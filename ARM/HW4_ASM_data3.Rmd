---
title: "ANLY 501 HW MOD4 data3"
author: "Leyao Li"
date: "10/21/2020"
output: html_document
---





```{r}
# install.packages('tokenizers')
# install.packages('arules')
library(arules)
library(tokenizers)
library(tidyverse)
library(plyr)
library(dplyr)
library(ggplot2)
tweet = read.csv('/Users/lingfengcao/Leyao/ANLY501/HW MOD4/tweets_trunc_V2_no_trump.csv')

tweet$text <- lapply(tweet$text, as.character)

# start a file
Trans <- file.create('TransactionTweetsFile_data3')
# open the file
Trans <- file('TransactionTweetsFile_data3', open = "a")

## Tokenize to words 
Tokens<-tokenizers::tokenize_words(
  tweet$text[701],stopwords = stopwords::stopwords("en"), 
  lowercase = TRUE,  strip_punct = TRUE, strip_numeric = TRUE,
  simplify = TRUE)

## Write tokens
cat(unlist(Tokens), "\n", file=Trans, sep=",")
close(Trans)

## Append remaining lists of tokens into file
## Recall - a list of tokens is the set of words from a Tweet
Trans <- file('TransactionTweetsFile_data3', open = "a")
for(i in 702:852){
  Tokens<-tokenize_words(tweet$text[i],stopwords = stopwords::stopwords("en"), 
                         lowercase = TRUE,  strip_punct = TRUE, simplify = TRUE)
  cat(unlist(Tokens), "\n", file=Trans, sep=",")
}
close(Trans)

######### Read in the tweet transactions
TweetTrans <- read.transactions('TransactionTweetsFile_data3',
                                rm.duplicates = FALSE,
                                format = "basket",
                                sep=","
                                ## cols =
)
# inspect(TweetTrans)
# See the words that occur the most
Sample_Trans <- sample(TweetTrans, 3)
summary(Sample_Trans)

```


```{r}
TweetDF <- read.csv('TransactionTweetsFile_data3', 
                    header = FALSE, sep = ",")
head(TweetDF)
(str(TweetDF))

## Convert all columns to char 
TweetDF<-TweetDF %>%
  mutate_all(as.character)
(str(TweetDF))

# remove words of choice
TweetDF[TweetDF == "t.co"] <- ""
TweetDF[TweetDF == "rt"] <- ""
TweetDF[TweetDF == "http"] <- ""
TweetDF[TweetDF == "https"] <- ""
TweetDF[TweetDF == "f"] <- ""
TweetDF[TweetDF == "h"] <- ""

```


```{r}
## Clean with grepl\
MyDF<-NULL
MyDF2<-NULL
for (i in 1:ncol(TweetDF)){
  MyList=c() 
  MyList2=c() 
  MyList=c(MyList,grepl("[[:digit:]]", TweetDF[[i]]))
  MyDF<-cbind(MyDF,MyList)  ## create a logical DF
  MyList2=c(MyList2,(nchar(TweetDF[[i]])<4 | nchar(TweetDF[[i]])>9))
  MyDF2<-cbind(MyDF2,MyList2) 
}
## For all TRUE, replace with blank
TweetDF[MyDF] <- ""
TweetDF[MyDF2] <- ""
(head(TweetDF,10))

# Now we save the dataframe using the write table command 
write.table(TweetDF, file = "UpdatedTweetFile_data3.csv", col.names = FALSE, 
            row.names = FALSE, sep = ",")
TweetTrans <- read.transactions("UpdatedTweetFile_data3.csv", sep =",", 
                                format("basket"),  rm.duplicates = TRUE)
```

```{r}
TweetTrans_rules = arules::apriori(TweetTrans, 
        parameter = list(support=.03, conf=1, minlen=2, maxlen=3))

summary(TweetTrans_rules[1:15])

##  Sort by Conf
SortedRules_conf <- sort(TweetTrans_rules, by="confidence", decreasing=TRUE)

inspect(SortedRules_conf[1:15])
## Sort by Sup
SortedRules_sup <- sort(TweetTrans_rules, by="support", decreasing=TRUE)
inspect(SortedRules_sup[1:15])
## Sort by Lift
SortedRules_lift <- sort(TweetTrans_rules, by="lift", decreasing=TRUE)
inspect(SortedRules_lift[1:15])
```
```{r}
TweetTrans_rules<-SortedRules_lift
inspect(TweetTrans_rules[1:15])
```
```{r}
## Convert the RULES to a DATAFRAME
Rules_DF2<-DATAFRAME(TweetTrans_rules, separate = TRUE)
(head(Rules_DF2))
str(Rules_DF2)
## Convert to char
Rules_DF2$LHS<-as.character(Rules_DF2$LHS)
Rules_DF2$RHS<-as.character(Rules_DF2$RHS)

## Remove all {}
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[{]', replacement='')
Rules_DF2[] <- lapply(Rules_DF2, gsub, pattern='[}]', replacement='')

head(Rules_DF2)
```
```{r}
###########################################
###### Do for SUp, Conf, and Lift   #######
###########################################
## Remove the sup, conf, and count
## USING LIFT
Rules_L<-Rules_DF2[c(1,2,5)]
names(Rules_L) <- c("SourceName", "TargetName", "Weight")
head(Rules_L,30)

## USING SUP
Rules_S<-Rules_DF2[c(1,2,3)]
names(Rules_S) <- c("SourceName", "TargetName", "Weight")
head(Rules_S,30)

## USING CONF
Rules_C<-Rules_DF2[c(1,2,4)]
names(Rules_C) <- c("SourceName", "TargetName", "Weight")
head(Rules_C,30)

## CHoose and set
#Rules_Sup<-Rules_C
Rules_Sup<-Rules_L
#Rules_Sup<-Rules_S
```
############################### BUILD THE NODES & EDGES ####################################


```{r}
# install.packages('arulesViz')
# install.packages('igraph')
# install.packages('httpuv')
# install.packages('openssl')
library(arulesViz)
library(igraph)

library(httpuv)
library(openssl)
(edgeList<-Rules_Sup)
MyGraph <- igraph::simplify(igraph::graph.data.frame(edgeList, directed=TRUE))

nodeList <- data.frame(ID = c(0:(igraph::vcount(MyGraph) - 1)), 
                       # because networkD3 library requires IDs to start at 0
                       nName = igraph::V(MyGraph)$name)
## Node Degree
(nodeList <- cbind(nodeList, nodeDegree=igraph::degree(MyGraph, 
                    v = igraph::V(MyGraph), mode = "all")))

## Betweenness
BetweenNess <- igraph::betweenness(MyGraph, 
      v = igraph::V(MyGraph), 
      directed = TRUE) 

(nodeList <- cbind(nodeList, nodeBetweenness=BetweenNess))

## This can change the BetweenNess value if needed
BetweenNess<-BetweenNess/100

###################################################################################
########## BUILD THE EDGES #####################################################
#############################################################
# Recall that ... 
# edgeList<-Rules_Sup
getNodeID <- function(x){
  which(x == igraph::V(MyGraph)$name) - 1  #IDs start at 0
}
(getNodeID("elephants")) 

edgeList <- plyr::ddply(
  Rules_Sup, .variables = c("SourceName", "TargetName" , "Weight"), 
  function (x) data.frame(SourceID = getNodeID(x$SourceName), 
                          TargetID = getNodeID(x$TargetName)))

head(edgeList)
nrow(edgeList)

#Calculate Dice similarities between all pairs of nodes
#The Dice similarity coefficient of two vertices is twice 
#the number of common neighbors divided by the sum of the degrees 
#of the vertices. Method dice calculates the pairwise Dice similarities 
#for some (or all) of the vertices. 
DiceSim <- igraph::similarity.dice(MyGraph, vids = igraph::V(MyGraph), mode = "all")
# head(DiceSim)

#Create  data frame that contains the Dice similarity between any two vertices
F1 <- function(x) {data.frame(diceSim = DiceSim[x$SourceID +1, x$TargetID + 1])}
#Place a new column in edgeList with the Dice Sim
# head(edgeList)

edgeList <- plyr::ddply(edgeList,
                        .variables=c("SourceName", "TargetName", "Weight",
                                      "SourceID", "TargetID"), 
                        function(x) data.frame(F1(x)))
# head(edgeList)
```


```{r}
COLOR_P <- colorRampPalette(c("#00FF00", "#FF0000"), 
                            bias = nrow(edgeList), space = "rgb", 
                            interpolate = "linear")
COLOR_P
(colCodes <- COLOR_P(length(unique(edgeList$diceSim))))
edges_col <- sapply(edgeList$diceSim, 
                    function(x) colCodes[which(sort(unique(edgeList$diceSim)) == x)])
nrow(edges_col)
```
```{r}
# install.packages('networkD3')
library(networkD3)
library(curl)
library(dplyr)
D3_network_Tweets <- networkD3::forceNetwork(
  Links = edgeList, # data frame that contains info about edges
  Nodes = nodeList, # data frame that contains info about nodes
  Source = "SourceID", # ID of source node 
  Target = "TargetID", # ID of target node
  Value = "Weight", # value from the edge list (data frame) that will be used to value/weight relationship amongst nodes
  NodeID = "nName", # value from the node list (data frame) that contains node description we want to use (e.g., node name)
  Nodesize = "nodeBetweenness",  # value from the node list (data frame) that contains value we want to use for a node size
  Group = "nodeDegree",  # value from the node list (data frame) that contains value we want to use for node color
  height = 700, # Size of the plot (vertical)
  width = 900,  # Size of the plot (horizontal)
  fontSize = 12, # Font size
  linkDistance = networkD3::JS("function(d) { return d.value*10; }"), # Function to determine distance between any two nodes, uses variables already defined in forceNetwork function (not variables from a data frame)
  linkWidth = networkD3::JS("function(d) { return d.value/10; }"),# Function to determine link/edge thickness, uses variables already defined in forceNetwork function (not variables from a data frame)
  opacity = 0.9, # opacity
  zoom = TRUE, # ability to zoom when click on the node
  opacityNoHover = 0.9, # opacity of labels when static
  linkColour = "red"   ###"edges_col"red"# edge colors
) 

# Plot network
#D3_network_Tweets

# Save network as html file
networkD3::saveNetwork(D3_network_Tweets, 
                       "NetD3_wildfireL_data3.html", selfcontained = TRUE)
```

Visualize the rules
```{r}
itemsets <- eclat(TweetTrans, parameter = list(support = 0.0
                                               
                                               , minlen=2))
plot(itemsets, method="graph")
```

frequency plot top 25
```{r}
itemFrequencyPlot(TweetTrans, topN = 25)
```